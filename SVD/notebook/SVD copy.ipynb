{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the data\n",
    "df = pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "df.info()\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATIONS:\n",
    "\n",
    "+ movieId: A unique identifier for the movie.\n",
    "+ title: The title of the movie, along with its release year in parentheses.\n",
    "+ genres: The genres associated with the movie, separated by pipe characters (|)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "\n",
      "Number of unique movies: 9724\n",
      "\n",
      "Number of unique ratings: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique users\n",
    "print(f'Number of unique users: {df.userId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique movies\n",
    "print(f'Number of unique movies: {df.movieId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {df.rating.unique().shape[0]}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replaces any infinities in the data with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values and infinities\n",
    "df.isnull().sum()\n",
    "df.isnull().values.any()\n",
    "# check for infinities\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits the data into a training set and a test set using a user-stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M50 set:\n",
      "        userId  movieId  rating   timestamp\n",
      "232         2      318     3.0  1445714835\n",
      "233         2      333     4.0  1445715029\n",
      "234         2     1704     4.5  1445715228\n",
      "235         2     3578     4.0  1445714885\n",
      "236         2     6874     4.0  1445714952\n",
      "...       ...      ...     ...         ...\n",
      "95096     599   179817     3.0  1516604716\n",
      "95097     599   180031     3.5  1518298493\n",
      "95098     599   180297     3.0  1516604804\n",
      "95099     599   181315     3.5  1517370374\n",
      "95100     599   183301     3.0  1519148271\n",
      "\n",
      "[11136 rows x 4 columns]\n",
      "M100 set:\n",
      "         userId  movieId  rating   timestamp\n",
      "261          3       31     0.5  1306463578\n",
      "262          3      527     0.5  1306464275\n",
      "263          3      647     0.5  1306463619\n",
      "264          3      688     0.5  1306464228\n",
      "265          3      720     0.5  1306463595\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[15572 rows x 4 columns]\n",
      "M400 set:\n",
      "        userId  movieId  rating  timestamp\n",
      "0           1        1     4.0  964982703\n",
      "1           1        3     4.0  964981247\n",
      "2           1        6     4.0  964982224\n",
      "3           1       47     5.0  964983815\n",
      "4           1       50     5.0  964982931\n",
      "...       ...      ...     ...        ...\n",
      "99529     609      892     3.0  847221080\n",
      "99530     609     1056     3.0  847221080\n",
      "99531     609     1059     3.0  847221054\n",
      "99532     609     1150     4.0  847221054\n",
      "99533     609     1161     4.0  847221080\n",
      "\n",
      "[66345 rows x 4 columns]\n",
      "Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "3781       23        6     4.0  1107342267\n",
      "3782       23       29     4.0  1107341574\n",
      "3783       23       32     3.5  1107341750\n",
      "3784       23       50     4.0  1107163741\n",
      "3785       23       58     3.0  1107164183\n",
      "...       ...      ...     ...         ...\n",
      "97359     605    74282     4.5  1277096867\n",
      "97360     605    74530     3.0  1277176182\n",
      "97361     605    76093     4.0  1277175655\n",
      "97362     605    76175     2.5  1277176136\n",
      "97363     605    78105     3.5  1277176533\n",
      "\n",
      "[7783 rows x 4 columns]\n",
      "Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "21452      140     4234     3.0  1012505945\n",
      "22899      156     2080     1.0   951113118\n",
      "58090      380   182639     4.0  1536874706\n",
      "79604      495     5254     3.5  1458636268\n",
      "100382     610    69134     3.0  1493848172\n",
      "...        ...      ...     ...         ...\n",
      "49818      318   136024     3.0  1455886225\n",
      "66934      432     5507     1.5  1315242710\n",
      "74191      474     4012     2.0  1046887657\n",
      "80857      510      497     1.5  1141158809\n",
      "40375      274    51709     3.5  1285897528\n",
      "\n",
      "[80668 rows x 4 columns]\n",
      "Test set:\n",
      "       userId  movieId  rating   timestamp\n",
      "0          1     1920     4.0   964981780\n",
      "1          1      457     5.0   964981909\n",
      "2          1     2648     4.0   964983414\n",
      "3          1      316     3.0   964982310\n",
      "4          1      661     5.0   964982838\n",
      "...      ...      ...     ...         ...\n",
      "5247     610     4020     3.5  1479542683\n",
      "5248     610    90600     3.5  1493847740\n",
      "5249     610    57669     5.0  1493845166\n",
      "5250     610     6287     3.0  1493847091\n",
      "5251     610     6620     4.0  1493845340\n",
      "\n",
      "[5252 rows x 4 columns]\n",
      "All-But-One Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100226 rows x 4 columns]\n",
      "All-But-One Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "219         1     3578     5.0   964980668\n",
      "245         2    77455     3.0  1445714941\n",
      "294         3     6835     5.0  1306463670\n",
      "306         4      106     4.0   986848784\n",
      "548         5      527     5.0   847434960\n",
      "...       ...      ...     ...         ...\n",
      "98227     606     7131     3.0  1171813499\n",
      "98506     607      423     3.0   963080410\n",
      "98707     608      188     3.5  1117503407\n",
      "99501     609      137     3.0   847221054\n",
      "99587     610      903     5.0  1479542931\n",
      "\n",
      "[610 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_rated_items(df, test_size, given_n):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df['userId'])\n",
    "\n",
    "    # For each user in the test set, keep only 'given_n' rated items if they have rated that many,\n",
    "    # otherwise keep all the items they have rated.\n",
    "    test_df = test_df.groupby('userId').apply(lambda x: x.sample(min(len(x), given_n), random_state=42))\n",
    "\n",
    "    return train_df, test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_data_by_unique_users(df):\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Get the user IDs for each set\n",
    "    M50_users = unique_users[:50]\n",
    "    M100_users = unique_users[50:150]\n",
    "    M400_users = unique_users[150:550]\n",
    "    test_users = unique_users[550:]\n",
    "\n",
    "    # Split the DataFrame into the different sets based on the user IDs\n",
    "    M50_df = df[df['userId'].isin(M50_users)]\n",
    "    M100_df = df[df['userId'].isin(M100_users)]\n",
    "    M400_df = df[df['userId'].isin(M400_users)]\n",
    "    test_df = df[df['userId'].isin(test_users)]\n",
    "\n",
    "    return M50_df, M100_df, M400_df, test_df\n",
    "\n",
    "\n",
    "def all_but_one(df):\n",
    "    # For each user, select one rating and split it into a separate DataFrame\n",
    "    test_df = df.groupby('userId').sample(n=1, random_state=42)\n",
    "    train_df = df.drop(test_df.index)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Call the function\n",
    "M50_df, M100_df, M400_df, test_df = split_data_by_unique_users(df)\n",
    "\n",
    "print('M50 set:\\n', M50_df)\n",
    "print('M100 set:\\n', M100_df)\n",
    "print('M400 set:\\n', M400_df)\n",
    "print('Test set:\\n', test_df)\n",
    "\n",
    "# Call the functions\n",
    "train_df_given_10, test_df_given_10 = split_data_by_rated_items(df, test_size=0.2, given_n=10)  # Modify test_size and given_n as needed\n",
    "print('Training set:\\n', train_df_given_10)\n",
    "print('Test set:\\n', test_df_given_10)\n",
    "\n",
    "train_df, test_df = all_but_one(df)\n",
    "print('All-But-One Training set:\\n', train_df)\n",
    "print('All-But-One Test set:\\n', test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "+ RMSE\n",
    "+ MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# rmse: root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return sqrt(mse)\n",
    "\n",
    "# mae: mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "# f1 score: harmonic mean of precision and recall\n",
    "def f1_score(precisions, recalls):\n",
    "    f1_scores = dict()\n",
    "    for uid in precisions.keys():\n",
    "        p, r = precisions[uid], recalls[uid]\n",
    "        f1_scores[uid] = 2*(p*r) / (p + r) if (p + r) != 0 else 0\n",
    "    return f1_scores\n",
    "\n",
    "# precision and recall at k\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "# ndcg at k: normalized discounted cumulative gain\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    ndcg_values = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        dcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(user_ratings[:k]))\n",
    "        idcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(sorted(user_ratings, key=lambda x: x[1], reverse=True)[:k]))\n",
    "        ndcg_values.append(dcg / idcg if idcg > 0.0 else 0.0)\n",
    "\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "\n",
    "# evaluate the model recording RMSE, MAE, precision and recall at k, F1 score, and NDCG at k metrics in a single dictionary\n",
    "def evaluate(predictions, k=10, threshold=3.5):\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=threshold)\n",
    "    f1_scores = f1_score(precisions, recalls)\n",
    "    ndcg = ndcg_at_k(predictions, k=k)\n",
    "    return {\n",
    "        'RMSE': rmse([true_r for uid, iid, true_r, est in predictions], [est for uid, iid, true_r, est in predictions]),\n",
    "        'MAE': mae([true_r for uid, iid, true_r, est in predictions], [est for uid, iid, true_r, est in predictions]),\n",
    "        'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "        'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "        'F1 score': sum(f1 for f1 in f1_scores.values()) / len(f1_scores),\n",
    "        'NDCG': ndcg\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "+ \"cold-start handling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, num_factors, learning_rate, num_epochs, top_n=10):\n",
    "        # Initializing the instance variables with given arguments\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.top_n = top_n  # number of movies to recommend for cold start\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        # Initializing the user and movie latent factors matrices with random numbers\n",
    "        self.user_factors = np.random.randn(user_item_ratings.userId.nunique(), self.num_factors)\n",
    "        self.movie_factors = np.random.randn(user_item_ratings.movieId.nunique(), self.num_factors)\n",
    "        \n",
    "        # Creating dictionaries to map user and movie IDs to their respective indices in the factor matrices\n",
    "        self.user_index = {user_id: idx for idx, user_id in enumerate(user_item_ratings.userId.unique())}\n",
    "        self.movie_index = {movie_id: idx for idx, movie_id in enumerate(user_item_ratings.movieId.unique())}\n",
    "\n",
    "        # Calculate average rating for each movie\n",
    "        self.movie_avg_rating = user_item_ratings.groupby('movieId')['rating'].mean().to_dict()\n",
    "\n",
    "        # Get top-N movies based on average rating for cold start problem\n",
    "        sorted_movies_by_avg_rating = sorted(self.movie_avg_rating.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.top_n_movies = [movie_id for movie_id, _ in sorted_movies_by_avg_rating[:self.top_n]]\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Loop over all user-item-rating rows in the DataFrame\n",
    "            for idx, row in user_item_ratings.iterrows():\n",
    "                user_id = row['userId']\n",
    "                movie_id = row['movieId']\n",
    "                rating = row['rating']\n",
    "\n",
    "                # Getting the user and movie indices for the current user-item pair\n",
    "                user_idx = self.user_index[user_id]\n",
    "                movie_idx = self.movie_index[movie_id]\n",
    "\n",
    "                # Computing the predicted rating as the dot product of the user and movie factors\n",
    "                prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "                # Computing the error as the difference between the actual and predicted ratings\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Updating the user and movie factor vectors in the direction that minimizes the error\n",
    "                self.user_factors[user_idx] += self.learning_rate * error * self.movie_factors[movie_idx]\n",
    "                self.movie_factors[movie_idx] += self.learning_rate * error * self.user_factors[user_idx]\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        # Getting the user and movie indices for the given user-item pair\n",
    "        user_idx = self.user_index.get(user_id, -1)\n",
    "        movie_idx = self.movie_index.get(movie_id, -1)\n",
    "\n",
    "        # If the user or the movie is not present in the training data, return the movie's average rating\n",
    "        if user_idx == -1 or movie_idx == -1:\n",
    "            return self.movie_avg_rating.get(movie_id)\n",
    "\n",
    "        # Otherwise, return the predicted rating as the dot product of the user and movie factors\n",
    "        return np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "\n",
    "    def recommend(self, user_id):\n",
    "        # If the user is not present in the training data, return top-N movies\n",
    "        if user_id not in self.user_index:\n",
    "            return self.top_n_movies\n",
    "\n",
    "        # Otherwise, predict the rating for each movie and return the top-N movies\n",
    "        user_ratings = {movie_id: self.predict(user_id, movie_id) for movie_id in self.movie_index.keys()}\n",
    "        sorted_user_ratings = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [movie_id for movie_id, _ in sorted_user_ratings[:self.top_n]]\n",
    "    \n",
    "\n",
    "svd = SVD(num_factors=35, learning_rate=0.01, num_epochs=10, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model):\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    test_predictions = test_df.apply(lambda row: model.predict(row['userId'], row['movieId']), axis=1)\n",
    "    \n",
    "    # Remove None values and corresponding actual ratings\n",
    "    actual_ratings = test_df['rating'][test_predictions.notna()]\n",
    "    test_predictions = test_predictions.dropna()\n",
    "\n",
    "    svd_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "    \n",
    "    # Compute metrics for the model\n",
    "    metrics = evaluate(svd_predictions)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Execution time for: {end_time - start_time} seconds\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for M50 dataset\n",
      "Execution time for: 4.63558292388916 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.0370570848555865, 'MAE': 0.7972022527624352, 'Precision@k': 0.8532110091743119, 'Recall@k': 0.8201834862385321, 'F1 score': 0.673394495412844, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M100 dataset\n",
      "Execution time for: 6.381701707839966 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.0089305769544652, 'MAE': 0.7463523189176293, 'Precision@k': 0.8173758865248227, 'Recall@k': 0.8847517730496454, 'F1 score': 0.7021276595744681, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M400 dataset\n",
      "Execution time for: 26.850507974624634 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 0.8534629144827903, 'MAE': 0.6111768750506389, 'Precision@k': 0.8923841059602649, 'Recall@k': 0.8509933774834437, 'F1 score': 0.7433774834437086, 'NDCG': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M50</td>\n",
       "      <td>1.037057</td>\n",
       "      <td>0.797202</td>\n",
       "      <td>0.853211</td>\n",
       "      <td>0.820183</td>\n",
       "      <td>0.673394</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M100</td>\n",
       "      <td>1.008931</td>\n",
       "      <td>0.746352</td>\n",
       "      <td>0.817376</td>\n",
       "      <td>0.884752</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M400</td>\n",
       "      <td>0.853463</td>\n",
       "      <td>0.611177</td>\n",
       "      <td>0.892384</td>\n",
       "      <td>0.850993</td>\n",
       "      <td>0.743377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset      RMSE       MAE  Precision@k  Recall@k  F1 score  NDCG\n",
       "0     M50  1.037057  0.797202     0.853211  0.820183  0.673394   1.0\n",
       "1    M100  1.008931  0.746352     0.817376  0.884752  0.702128   1.0\n",
       "2    M400  0.853463  0.611177     0.892384  0.850993  0.743377   1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Execution time for M50 dataset\")\n",
    "svd_metrics_M50 = evaluate_model(M50_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M50}\\n\")\n",
    "\n",
    "print(f\"Execution time for M100 dataset\")\n",
    "svd_metrics_M100 = evaluate_model(M100_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M100}\\n\")\n",
    "\n",
    "print(f\"Execution time for M400 dataset\")\n",
    "svd_metrics_M400 = evaluate_model(M400_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M400}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the metrics to DataFrame\n",
    "df_M50 = pd.DataFrame([svd_metrics_M50])\n",
    "df_M50['Dataset'] = 'M50'\n",
    "\n",
    "df_M100 = pd.DataFrame([svd_metrics_M100])\n",
    "df_M100['Dataset'] = 'M100'\n",
    "\n",
    "df_M400 = pd.DataFrame([svd_metrics_M400])\n",
    "df_M400['Dataset'] = 'M400'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "metrics_df = pd.concat([df_M50, df_M100, df_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]  # Move the last column to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CF:\n",
    "    def __init__(self, n_users, n_items, k=3, gamma=0, delta=25, epsilon=1e-9):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.user_corrs = np.zeros((n_users, n_users))\n",
    "        self.item_corrs = np.zeros((n_items, n_items))\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # user-based\n",
    "        for i in range(self.n_users):\n",
    "            for j in range(self.n_users):\n",
    "                self.user_corrs[i, j] = self.pearson_corr(user_item_matrix[i], user_item_matrix[j])\n",
    "\n",
    "        # item-based\n",
    "        for i in range(self.n_items):\n",
    "            for j in range(self.n_items):\n",
    "                self.item_corrs[i, j] = self.pearson_corr(user_item_matrix[:, i], user_item_matrix[:, j])\n",
    "\n",
    "    def predict(self, user_item_matrix, mode='user'):\n",
    "        predictions = np.zeros((self.n_users, self.n_items))\n",
    "        if mode == 'user':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_users = np.argsort(self.user_corrs[i])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_users, i, j, mode)\n",
    "        elif mode == 'item':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_items = np.argsort(self.item_corrs[j])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_items, i, j, mode)\n",
    "        return predictions\n",
    "\n",
    "    def pearson_corr(self, vec_i, vec_j):\n",
    "        mask_i = vec_i > 0\n",
    "        mask_j = vec_j > 0\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            return 0\n",
    "        mean_i = np.mean(vec_i[corrated_index])\n",
    "        mean_j = np.mean(vec_j[corrated_index])\n",
    "        sub_i = vec_i[corrated_index] - mean_i\n",
    "        sub_j = vec_j[corrated_index] - mean_j\n",
    "        return np.sum(sub_i * sub_j) / (np.sqrt(np.sum(np.square(sub_i))) * np.sqrt(np.sum(np.square(sub_j))) + self.epsilon)\n",
    "\n",
    "    def predict_rating(self, user_item_matrix, sim_indices, i, j, mode):\n",
    "        if mode == 'user':\n",
    "            sim_ratings = user_item_matrix[sim_indices, j]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[k][user_item_matrix[k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.user_corrs[i][sim_indices]\n",
    "        elif mode == 'item':\n",
    "            sim_ratings = user_item_matrix[i, sim_indices]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[:, k][user_item_matrix[:, k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.item_corrs[j][sim_indices]\n",
    "        if np.sum(sim_vals) == 0:\n",
    "            return np.mean(sim_ratings)\n",
    "        else:\n",
    "            return np.mean(sim_ratings) + np.sum(sim_vals * (sim_ratings - sim_means)) / np.sum(sim_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bt/7ws71xxn45bffcbhj7ddvm080000gn/T/ipykernel_49668/1421912638.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
      "/var/folders/bt/7ws71xxn45bffcbhj7ddvm080000gn/T/ipykernel_49668/1421912638.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 7.684165372697635,\n",
       " 'MAE': 7.598142305759955,\n",
       " 'Precision@k': 1.0,\n",
       " 'Recall@k': 0.27,\n",
       " 'F1 score': 0.27,\n",
       " 'NDCG': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_matrix(df, nrows, ncols):\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "    for row in df.itertuples():\n",
    "        matrix[row.userId, row.movieId] = row.rating\n",
    "    return matrix\n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M100_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M100_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
    "M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M100_df['userId'].nunique()\n",
    "n_items = M100_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M100_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "# print('Test RMSE (M50):', sqrt(mean_squared_error(actual_ratings, test_predictions)))\n",
    "# print('Test MAE (M50):', mean_absolute_error(actual_ratings, test_predictions))\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics_M100 = evaluate(knn_predictions)\n",
    "knn_metrics_M100\n",
    "\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics = evaluate(knn_predictions)\n",
    "knn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 7.684165372697635,\n",
       " 'MAE': 7.598142305759955,\n",
       " 'Precision@k': 1.0,\n",
       " 'Recall@k': 0.27,\n",
       " 'F1 score': 0.27,\n",
       " 'NDCG': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics = evaluate(knn_predictions)\n",
    "knn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def process_dataframe(df):\n",
    "#     df = df.copy()  # Add this line\n",
    "\n",
    "#     # Create mappings for userIds and movieIds to contiguous indices\n",
    "#     user_mapping = {user_id: i for i, user_id in enumerate(df['userId'].unique())}\n",
    "#     movie_mapping = {movie_id: i for i, movie_id in enumerate(df['movieId'].unique())}\n",
    "\n",
    "#     # Apply the mappings to the dataframe\n",
    "#     df.loc[:, 'userId'] = df['userId'].map(user_mapping)\n",
    "#     df.loc[:, 'movieId'] = df['movieId'].map(movie_mapping)\n",
    "\n",
    "#     # Drop rows with NaN userId or movieId\n",
    "#     df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "#     # Convert userId and movieId to integer\n",
    "#     df.loc[:, 'userId'] = df['userId'].astype(int)\n",
    "#     df.loc[:, 'movieId'] = df['movieId'].astype(int)\n",
    "    \n",
    "#     return df, user_mapping, movie_mapping\n",
    "\n",
    "\n",
    "\n",
    "# # Process and train on M100_df\n",
    "# M100_df, user_mapping, movie_mapping = process_dataframe(M100_df)\n",
    "# test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "# test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "# knn_metrics_M100 = train_knn(M100_df, test_df)\n",
    "\n",
    "# # Process and train on M50_df\n",
    "# M50_df, user_mapping, movie_mapping = process_dataframe(M50_df)\n",
    "# test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "# test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "# knn_metrics_M50 = train_knn(M50_df, test_df)\n",
    "\n",
    "# # Process and train on M400_df\n",
    "# M400_df, user_mapping, movie_mapping = process_dataframe(M400_df)\n",
    "# test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "# test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "# knn_metrics_M400 = train_knn(M400_df, test_df)\n",
    "\n",
    "# print(\"KNN Metrics for M100: \", knn_metrics_M100)\n",
    "# print(\"KNN Metrics for M50: \", knn_metrics_M50)\n",
    "# print(\"KNN Metrics for M400: \", knn_metrics_M400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    df = df.copy()  # Add this line\n",
    "\n",
    "    # Create mappings for userIds and movieIds to contiguous indices\n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(df['userId'].unique())}\n",
    "    movie_mapping = {movie_id: i for i, movie_id in enumerate(df['movieId'].unique())}\n",
    "\n",
    "    # Apply the mappings to the dataframe\n",
    "    df.loc[:, 'userId'] = df['userId'].map(user_mapping)\n",
    "    df.loc[:, 'movieId'] = df['movieId'].map(movie_mapping)\n",
    "\n",
    "    # Drop rows with NaN userId or movieId\n",
    "    df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "    # Convert userId and movieId to integer\n",
    "    df.loc[:, 'userId'] = df['userId'].astype(int)\n",
    "    df.loc[:, 'movieId'] = df['movieId'].astype(int)\n",
    "    \n",
    "    return df, user_mapping, movie_mapping\n",
    "\n",
    "def evaluate_model(df, model, dataset_name):\n",
    "    \n",
    "    # Copy df and apply processing\n",
    "    df, user_mapping, movie_mapping = process_dataframe(df)\n",
    "    \n",
    "    # Apply mappings to test_df\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['userId'] = test_df_copy['userId'].map(user_mapping)\n",
    "    test_df_copy['movieId'] = test_df_copy['movieId'].map(movie_mapping)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model_metrics = train_knn(df, test_df_copy)\n",
    "    \n",
    "    # Add execution time to metrics\n",
    "    exec_time = time.time() - start_time\n",
    "    model_metrics['Execution time'] = exec_time\n",
    "    \n",
    "    # Add dataset name to metrics\n",
    "    model_metrics['Dataset'] = dataset_name\n",
    "    \n",
    "    # Convert metrics to DataFrame\n",
    "    model_metrics_df = pd.DataFrame([model_metrics])\n",
    "    \n",
    "    return model_metrics_df\n",
    "\n",
    "def train_knn(df, test_df, k=3):\n",
    "    n_users = df['userId'].nunique()\n",
    "    n_items = df['movieId'].nunique()\n",
    "\n",
    "    train_matrix = df_to_matrix(df, n_users, n_items)\n",
    "    test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "    knn_cf = KNN_CF(n_users, n_items, k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    knn_cf.fit(train_matrix)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "    test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "    actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "    \n",
    "    knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "\n",
    "    # Compute metrics for the KNN model\n",
    "    knn_metrics = evaluate(knn_predictions)\n",
    "\n",
    "    return knn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 23\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(df, model, dataset_name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m test_df_copy[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_df_copy[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(movie_mapping)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Train and evaluate model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model_metrics \u001b[39m=\u001b[39m train_knn(df, test_df_copy)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Add execution time to metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m exec_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 23\u001b[0m in \u001b[0;36mtrain_knn\u001b[0;34m(df, test_df, k)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m n_items \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m train_matrix \u001b[39m=\u001b[39m df_to_matrix(df, n_users, n_items)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m test_matrix \u001b[39m=\u001b[39m df_to_matrix(test_df, n_users, n_items)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m knn_cf \u001b[39m=\u001b[39m KNN_CF(n_users, n_items, k)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Fit the model to the data\u001b[39;00m\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 23\u001b[0m in \u001b[0;36mdf_to_matrix\u001b[0;34m(df, nrows, ncols)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((nrows, ncols))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mitertuples():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     matrix[row\u001b[39m.\u001b[39muserId, row\u001b[39m.\u001b[39mmovieId] \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mrating\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m matrix\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate models for each dataset\n",
    "metrics_M50 = evaluate_model(M50_df, KNN_CF(n_users=M50_df['userId'].nunique(), n_items=M50_df['movieId'].nunique(), k=3), 'M50')\n",
    "metrics_M100 = evaluate_model(M100_df, KNN_CF(n_users=M100_df['userId'].nunique(), n_items=M100_df['movieId'].nunique(), k=3), 'M100')\n",
    "metrics_M400 = evaluate_model(M400_df, KNN_CF(n_users=M400_df['userId'].nunique(), n_items=M400_df['movieId'].nunique(), k=3), 'M400')\n",
    "\n",
    "# Concatenate metrics DataFrames\n",
    "metrics_df = pd.concat([metrics_M50, metrics_M100, metrics_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]  # Move the last two columns to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_knn(df, test_df, k=3):\n",
    "#     # Start time\n",
    "#     start_time = time.time()\n",
    "#     n_users = df['userId'].nunique()\n",
    "#     n_items = df['movieId'].nunique()\n",
    "\n",
    "#     train_matrix = df_to_matrix(df, n_users, n_items)\n",
    "#     test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "#     knn_cf = KNN_CF(n_users, n_items, k)\n",
    "\n",
    "#     # Fit the model to the data\n",
    "#     knn_cf.fit(train_matrix)\n",
    "\n",
    "#     # Predict ratings for the Test set and evaluate\n",
    "#     user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "#     test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "#     actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "#     # Remove NaN and inf values\n",
    "#     clean_predictions = test_predictions[~np.isnan(test_predictions) & ~np.isinf(test_predictions)]\n",
    "#     clean_actual_ratings = actual_ratings[~np.isnan(actual_ratings) & ~np.isinf(actual_ratings)]\n",
    "    \n",
    "#     knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], clean_actual_ratings, clean_predictions)]\n",
    "\n",
    "#     # Compute metrics for the KNN model\n",
    "#     knn_metrics = evaluate(knn_predictions)\n",
    "\n",
    "#     return knn_metrics\n",
    "\n",
    "\n",
    "# def evaluate(predictions, k=10, threshold=3.5):\n",
    "#     precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=threshold)\n",
    "#     f1_scores = f1_score(precisions, recalls)\n",
    "#     ndcg = ndcg_at_k(predictions, k=k)\n",
    "\n",
    "#     y_true = [true_r for uid, iid, true_r, est in predictions]\n",
    "#     y_pred = [est for uid, iid, true_r, est in predictions]\n",
    "\n",
    "#     # Remove NaN and inf values\n",
    "#     y_true_clean = np.array(y_true)[~np.isnan(y_true) & ~np.isinf(y_true)]\n",
    "#     y_pred_clean = np.array(y_pred)[~np.isnan(y_pred) & ~np.isinf(y_pred)]\n",
    "    \n",
    "#     return {\n",
    "#         'RMSE': rmse(y_true_clean, y_pred_clean),\n",
    "#         'MAE': mae(y_true_clean, y_pred_clean),\n",
    "#         'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "#         'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "#         'F1 score': sum(f1 for f1 in f1_scores.values()) / len(f1_scores),\n",
    "#         'NDCG': ndcg\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_dropna(df, user_mapping, movie_mapping):\n",
    "    df = df.copy()\n",
    "    df['userId'] = df['userId'].map(user_mapping)\n",
    "    df['movieId'] = df['movieId'].map(movie_mapping)\n",
    "    df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "    \n",
    "    # Convert to integer type if they are not NaN\n",
    "    df.loc[df['userId'].notna(), 'userId'] = df.loc[df['userId'].notna(), 'userId'].astype(int)\n",
    "    df.loc[df['movieId'].notna(), 'movieId'] = df.loc[df['movieId'].notna(), 'movieId'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Process and train on M100_df\n",
    "M100_df, user_mapping, movie_mapping = process_dataframe(M100_df)\n",
    "test_df_M100 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M100 = train_knn(M100_df, test_df_M100)\n",
    "\n",
    "# Process and train on M50_df\n",
    "M50_df, user_mapping, movie_mapping = process_dataframe(M50_df)\n",
    "test_df_M50 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M50 = train_knn(M50_df, test_df_M50)\n",
    "\n",
    "# Process and train on M400_df\n",
    "M400_df, user_mapping, movie_mapping = process_dataframe(M400_df)\n",
    "test_df_M400 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M400 = train_knn(M400_df, test_df_M400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
