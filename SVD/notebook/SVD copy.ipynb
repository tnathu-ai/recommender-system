{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the data\n",
    "df = pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "df.info()\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATIONS:\n",
    "\n",
    "+ movieId: A unique identifier for the movie.\n",
    "+ title: The title of the movie, along with its release year in parentheses.\n",
    "+ genres: The genres associated with the movie, separated by pipe characters (|)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "\n",
      "Number of unique movies: 9724\n",
      "\n",
      "Number of unique ratings: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique users\n",
    "print(f'Number of unique users: {df.userId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique movies\n",
    "print(f'Number of unique movies: {df.movieId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {df.rating.unique().shape[0]}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replaces any infinities in the data with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values and infinities\n",
    "df.isnull().sum()\n",
    "df.isnull().values.any()\n",
    "# check for infinities\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits the data into a training set and a test set using a user-stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M50 set:\n",
      "        userId  movieId  rating  timestamp\n",
      "5377       38       11     5.0  841341447\n",
      "5378       38       17     3.0  841341494\n",
      "5379       38       21     3.0  841341362\n",
      "5380       38       39     3.0  841341384\n",
      "5381       38       48     3.0  841341570\n",
      "...       ...      ...     ...        ...\n",
      "97138     604      636     3.0  832080690\n",
      "97139     604      637     4.0  832081130\n",
      "97140     604      708     3.0  832080461\n",
      "97141     604      724     3.0  832080735\n",
      "97142     604      742     4.0  832080636\n",
      "\n",
      "[5330 rows x 4 columns]\n",
      "M100 set:\n",
      "        userId  movieId  rating   timestamp\n",
      "0           1        1     4.0   964982703\n",
      "1           1        3     4.0   964981247\n",
      "2           1        6     4.0   964982224\n",
      "3           1       47     5.0   964983815\n",
      "4           1       50     5.0   964982931\n",
      "...       ...      ...     ...         ...\n",
      "91723     594     8727     2.0  1108950862\n",
      "91724     594     8778     4.0  1108951196\n",
      "91725     594     8866     4.5  1108975669\n",
      "91726     594     8869     5.0  1108976076\n",
      "91727     594    31433     4.5  1129084042\n",
      "\n",
      "[14477 rows x 4 columns]\n",
      "M400 set:\n",
      "         userId  movieId  rating   timestamp\n",
      "261          3       31     0.5  1306463578\n",
      "262          3      527     0.5  1306464275\n",
      "263          3      647     0.5  1306463619\n",
      "264          3      688     0.5  1306464228\n",
      "265          3      720     0.5  1306463595\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[70876 rows x 4 columns]\n",
      "Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "1323       12       39     4.0  1247264471\n",
      "1324       12      168     5.0  1247264411\n",
      "1325       12      222     5.0  1247263465\n",
      "1326       12      256     5.0  1247263375\n",
      "1327       12      261     4.5  1247263357\n",
      "...       ...      ...     ...         ...\n",
      "92154     596   181719     3.5  1535832847\n",
      "92155     596   182793     3.5  1535832710\n",
      "92156     596   183635     3.5  1535709593\n",
      "92157     596   184997     4.0  1535828629\n",
      "92158     596   188301     4.0  1535709468\n",
      "\n",
      "[10153 rows x 4 columns]\n",
      "Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "21452      140     4234     3.0  1012505945\n",
      "22899      156     2080     1.0   951113118\n",
      "58090      380   182639     4.0  1536874706\n",
      "79604      495     5254     3.5  1458636268\n",
      "100382     610    69134     3.0  1493848172\n",
      "...        ...      ...     ...         ...\n",
      "49818      318   136024     3.0  1455886225\n",
      "66934      432     5507     1.5  1315242710\n",
      "74191      474     4012     2.0  1046887657\n",
      "80857      510      497     1.5  1141158809\n",
      "40375      274    51709     3.5  1285897528\n",
      "\n",
      "[80668 rows x 4 columns]\n",
      "Test set:\n",
      "       userId  movieId  rating   timestamp\n",
      "0          1     1920     4.0   964981780\n",
      "1          1      457     5.0   964981909\n",
      "2          1     2648     4.0   964983414\n",
      "3          1      316     3.0   964982310\n",
      "4          1      661     5.0   964982838\n",
      "...      ...      ...     ...         ...\n",
      "5247     610     4020     3.5  1479542683\n",
      "5248     610    90600     3.5  1493847740\n",
      "5249     610    57669     5.0  1493845166\n",
      "5250     610     6287     3.0  1493847091\n",
      "5251     610     6620     4.0  1493845340\n",
      "\n",
      "[5252 rows x 4 columns]\n",
      "All-But-One Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100226 rows x 4 columns]\n",
      "All-But-One Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "219         1     3578     5.0   964980668\n",
      "245         2    77455     3.0  1445714941\n",
      "294         3     6835     5.0  1306463670\n",
      "306         4      106     4.0   986848784\n",
      "548         5      527     5.0   847434960\n",
      "...       ...      ...     ...         ...\n",
      "98227     606     7131     3.0  1171813499\n",
      "98506     607      423     3.0   963080410\n",
      "98707     608      188     3.5  1117503407\n",
      "99501     609      137     3.0   847221054\n",
      "99587     610      903     5.0  1479542931\n",
      "\n",
      "[610 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_rated_items(df, test_size, given_n):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df['userId'])\n",
    "\n",
    "    # For each user in the test set, keep only 'given_n' rated items if they have rated that many,\n",
    "    # otherwise keep all the items they have rated.\n",
    "    test_df = test_df.groupby('userId').apply(lambda x: x.sample(min(len(x), given_n), random_state=42))\n",
    "\n",
    "    return train_df, test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_data_by_unique_users(df):\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Get the user IDs for each set\n",
    "    M50_users = unique_users[:50]\n",
    "    M100_users = unique_users[50:150]\n",
    "    M400_users = unique_users[150:550]\n",
    "    test_users = unique_users[550:]\n",
    "\n",
    "    # Split the DataFrame into the different sets based on the user IDs\n",
    "    M50_df = df[df['userId'].isin(M50_users)]\n",
    "    M100_df = df[df['userId'].isin(M100_users)]\n",
    "    M400_df = df[df['userId'].isin(M400_users)]\n",
    "    test_df = df[df['userId'].isin(test_users)]\n",
    "\n",
    "    return M50_df, M100_df, M400_df, test_df\n",
    "\n",
    "\n",
    "def all_but_one(df):\n",
    "    # For each user, select one rating and split it into a separate DataFrame\n",
    "    test_df = df.groupby('userId').sample(n=1, random_state=42)\n",
    "    train_df = df.drop(test_df.index)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Call the function\n",
    "M50_df, M100_df, M400_df, test_df = split_data_by_unique_users(df)\n",
    "\n",
    "print('M50 set:\\n', M50_df)\n",
    "print('M100 set:\\n', M100_df)\n",
    "print('M400 set:\\n', M400_df)\n",
    "print('Test set:\\n', test_df)\n",
    "\n",
    "# Call the functions\n",
    "train_df_given_10, test_df_given_10 = split_data_by_rated_items(df, test_size=0.2, given_n=10)  # Modify test_size and given_n as needed\n",
    "print('Training set:\\n', train_df_given_10)\n",
    "print('Test set:\\n', test_df_given_10)\n",
    "\n",
    "train_df, test_df = all_but_one(df)\n",
    "print('All-But-One Training set:\\n', train_df)\n",
    "print('All-But-One Test set:\\n', test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "+ RMSE\n",
    "+ MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# rmse: root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return sqrt(mse)\n",
    "\n",
    "# mae: mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "# f1 score: harmonic mean of precision and recall\n",
    "def f1_score(precisions, recalls):\n",
    "    f1_scores = dict()\n",
    "    for uid in precisions.keys():\n",
    "        p, r = precisions[uid], recalls[uid]\n",
    "        f1_scores[uid] = 2*(p*r) / (p + r) if (p + r) != 0 else 0\n",
    "    return f1_scores\n",
    "\n",
    "# precision and recall at k\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "# ndcg at k: normalized discounted cumulative gain\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    ndcg_values = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        dcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(user_ratings[:k]))\n",
    "        idcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(sorted(user_ratings, key=lambda x: x[1], reverse=True)[:k]))\n",
    "        ndcg_values.append(dcg / idcg if idcg > 0.0 else 0.0)\n",
    "\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "\n",
    "# evaluate the model recording RMSE, MAE, precision and recall at k, F1 score, and NDCG at k metrics in a single dictionary\n",
    "def evaluate(predictions, k=10, threshold=3.5):\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=threshold)\n",
    "    f1_scores = f1_score(precisions, recalls)\n",
    "    ndcg = ndcg_at_k(predictions, k=k)\n",
    "    return {\n",
    "        'RMSE': rmse([true_r for uid, iid, true_r, est in predictions], [est for uid, iid, true_r, est in predictions]),\n",
    "        'MAE': mae([true_r for uid, iid, true_r, est in predictions], [est for uid, iid, true_r, est in predictions]),\n",
    "        'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "        'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "        'F1 score': sum(f1 for f1 in f1_scores.values()) / len(f1_scores),\n",
    "        'NDCG': ndcg\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "+ \"cold-start handling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, num_factors, learning_rate, num_epochs, top_n=10):\n",
    "        # Initializing the instance variables with given arguments\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.top_n = top_n  # number of movies to recommend for cold start\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        # Initializing the user and movie latent factors matrices with random numbers\n",
    "        self.user_factors = np.random.randn(user_item_ratings.userId.nunique(), self.num_factors)\n",
    "        self.movie_factors = np.random.randn(user_item_ratings.movieId.nunique(), self.num_factors)\n",
    "        \n",
    "        # Creating dictionaries to map user and movie IDs to their respective indices in the factor matrices\n",
    "        self.user_index = {user_id: idx for idx, user_id in enumerate(user_item_ratings.userId.unique())}\n",
    "        self.movie_index = {movie_id: idx for idx, movie_id in enumerate(user_item_ratings.movieId.unique())}\n",
    "\n",
    "        # Calculate average rating for each movie\n",
    "        self.movie_avg_rating = user_item_ratings.groupby('movieId')['rating'].mean().to_dict()\n",
    "\n",
    "        # Get top-N movies based on average rating for cold start problem\n",
    "        sorted_movies_by_avg_rating = sorted(self.movie_avg_rating.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.top_n_movies = [movie_id for movie_id, _ in sorted_movies_by_avg_rating[:self.top_n]]\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Loop over all user-item-rating rows in the DataFrame\n",
    "            for idx, row in user_item_ratings.iterrows():\n",
    "                user_id = row['userId']\n",
    "                movie_id = row['movieId']\n",
    "                rating = row['rating']\n",
    "\n",
    "                # Getting the user and movie indices for the current user-item pair\n",
    "                user_idx = self.user_index[user_id]\n",
    "                movie_idx = self.movie_index[movie_id]\n",
    "\n",
    "                # Computing the predicted rating as the dot product of the user and movie factors\n",
    "                prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "                # Computing the error as the difference between the actual and predicted ratings\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Updating the user and movie factor vectors in the direction that minimizes the error\n",
    "                self.user_factors[user_idx] += self.learning_rate * error * self.movie_factors[movie_idx]\n",
    "                self.movie_factors[movie_idx] += self.learning_rate * error * self.user_factors[user_idx]\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        # Getting the user and movie indices for the given user-item pair\n",
    "        user_idx = self.user_index.get(user_id, -1)\n",
    "        movie_idx = self.movie_index.get(movie_id, -1)\n",
    "\n",
    "        # If the user or the movie is not present in the training data, return the movie's average rating\n",
    "        if user_idx == -1 or movie_idx == -1:\n",
    "            return self.movie_avg_rating.get(movie_id)\n",
    "\n",
    "        # Otherwise, return the predicted rating as the dot product of the user and movie factors\n",
    "        return np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "\n",
    "    def recommend(self, user_id):\n",
    "        # If the user is not present in the training data, return top-N movies\n",
    "        if user_id not in self.user_index:\n",
    "            return self.top_n_movies\n",
    "\n",
    "        # Otherwise, predict the rating for each movie and return the top-N movies\n",
    "        user_ratings = {movie_id: self.predict(user_id, movie_id) for movie_id in self.movie_index.keys()}\n",
    "        sorted_user_ratings = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [movie_id for movie_id, _ in sorted_user_ratings[:self.top_n]]\n",
    "    \n",
    "\n",
    "svd = SVD(num_factors=35, learning_rate=0.01, num_epochs=10, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model):\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    test_predictions = test_df.apply(lambda row: model.predict(row['userId'], row['movieId']), axis=1)\n",
    "    \n",
    "    # Remove None values and corresponding actual ratings\n",
    "    actual_ratings = test_df['rating'][test_predictions.notna()]\n",
    "    test_predictions = test_predictions.dropna()\n",
    "\n",
    "    svd_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "    \n",
    "    # Compute metrics for the model\n",
    "    metrics = evaluate(svd_predictions)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Execution time for: {end_time - start_time} seconds\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for M50 dataset\n",
      "Execution time for: 2.2075259685516357 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.041359842194938, 'MAE': 0.7931285610571793, 'Precision@k': 0.8024193548387096, 'Recall@k': 0.842741935483871, 'F1 score': 0.6451612903225806, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M100 dataset\n",
      "Execution time for: 5.991877794265747 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.0235664676943952, 'MAE': 0.7638649666840859, 'Precision@k': 0.8512544802867383, 'Recall@k': 0.8243727598566308, 'F1 score': 0.6756272401433692, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M400 dataset\n",
      "Execution time for: 28.712896823883057 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 0.8189564361794365, 'MAE': 0.5815151122352734, 'Precision@k': 0.9139072847682119, 'Recall@k': 0.8609271523178808, 'F1 score': 0.7748344370860927, 'NDCG': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M50</td>\n",
       "      <td>1.041360</td>\n",
       "      <td>0.793129</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M100</td>\n",
       "      <td>1.023566</td>\n",
       "      <td>0.763865</td>\n",
       "      <td>0.851254</td>\n",
       "      <td>0.824373</td>\n",
       "      <td>0.675627</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M400</td>\n",
       "      <td>0.818956</td>\n",
       "      <td>0.581515</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.774834</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset      RMSE       MAE  Precision@k  Recall@k  F1 score  NDCG\n",
       "0     M50  1.041360  0.793129     0.802419  0.842742  0.645161   1.0\n",
       "1    M100  1.023566  0.763865     0.851254  0.824373  0.675627   1.0\n",
       "2    M400  0.818956  0.581515     0.913907  0.860927  0.774834   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Execution time for M50 dataset\")\n",
    "svd_metrics_M50 = evaluate_model(M50_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M50}\\n\")\n",
    "\n",
    "print(f\"Execution time for M100 dataset\")\n",
    "svd_metrics_M100 = evaluate_model(M100_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M100}\\n\")\n",
    "\n",
    "print(f\"Execution time for M400 dataset\")\n",
    "svd_metrics_M400 = evaluate_model(M400_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M400}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the metrics to DataFrame\n",
    "df_M50 = pd.DataFrame([svd_metrics_M50])\n",
    "df_M50['Dataset'] = 'M50'\n",
    "\n",
    "df_M100 = pd.DataFrame([svd_metrics_M100])\n",
    "df_M100['Dataset'] = 'M100'\n",
    "\n",
    "df_M400 = pd.DataFrame([svd_metrics_M400])\n",
    "df_M400['Dataset'] = 'M400'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "metrics_df = pd.concat([df_M50, df_M100, df_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]  # Move the last column to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CF:\n",
    "    def __init__(self, n_users, n_items, k=3, gamma=0, delta=25, epsilon=1e-9):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.user_corrs = np.zeros((n_users, n_users))\n",
    "        self.item_corrs = np.zeros((n_items, n_items))\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # user-based\n",
    "        for i in range(self.n_users):\n",
    "            for j in range(self.n_users):\n",
    "                self.user_corrs[i, j] = self.pearson_corr(user_item_matrix[i], user_item_matrix[j])\n",
    "\n",
    "        # item-based\n",
    "        for i in range(self.n_items):\n",
    "            for j in range(self.n_items):\n",
    "                self.item_corrs[i, j] = self.pearson_corr(user_item_matrix[:, i], user_item_matrix[:, j])\n",
    "\n",
    "    def predict(self, user_item_matrix, mode='user'):\n",
    "        predictions = np.zeros((self.n_users, self.n_items))\n",
    "        if mode == 'user':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_users = np.argsort(self.user_corrs[i])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_users, i, j, mode)\n",
    "        elif mode == 'item':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_items = np.argsort(self.item_corrs[j])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_items, i, j, mode)\n",
    "        return predictions\n",
    "\n",
    "    def pearson_corr(self, vec_i, vec_j):\n",
    "        mask_i = vec_i > 0\n",
    "        mask_j = vec_j > 0\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            return 0\n",
    "        mean_i = np.mean(vec_i[corrated_index])\n",
    "        mean_j = np.mean(vec_j[corrated_index])\n",
    "        sub_i = vec_i[corrated_index] - mean_i\n",
    "        sub_j = vec_j[corrated_index] - mean_j\n",
    "        return np.sum(sub_i * sub_j) / (np.sqrt(np.sum(np.square(sub_i))) * np.sqrt(np.sum(np.square(sub_j))) + self.epsilon)\n",
    "\n",
    "    def predict_rating(self, user_item_matrix, sim_indices, i, j, mode):\n",
    "        if mode == 'user':\n",
    "            sim_ratings = user_item_matrix[sim_indices, j]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[k][user_item_matrix[k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.user_corrs[i][sim_indices]\n",
    "        elif mode == 'item':\n",
    "            sim_ratings = user_item_matrix[i, sim_indices]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[:, k][user_item_matrix[:, k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.item_corrs[j][sim_indices]\n",
    "        if np.sum(sim_vals) == 0:\n",
    "            return np.mean(sim_ratings)\n",
    "        else:\n",
    "            return np.mean(sim_ratings) + np.sum(sim_vals * (sim_ratings - sim_means)) / np.sum(sim_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bt/7ws71xxn45bffcbhj7ddvm080000gn/T/ipykernel_48416/2973701214.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
      "/var/folders/bt/7ws71xxn45bffcbhj7ddvm080000gn/T/ipykernel_48416/2973701214.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'knn_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X51sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Compute metrics for the KNN model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X51sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m knn_metrics_ \u001b[39m=\u001b[39m evaluate(knn_predictions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X51sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m knn_metrics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'knn_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "def df_to_matrix(df, nrows, ncols):\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "    for row in df.itertuples():\n",
    "        matrix[row.userId, row.movieId] = row.rating\n",
    "    return matrix\n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M100_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M100_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
    "M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M100_df['userId'].nunique()\n",
    "n_items = M100_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M100_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "# print('Test RMSE (M50):', sqrt(mean_squared_error(actual_ratings, test_predictions)))\n",
    "# print('Test MAE (M50):', mean_absolute_error(actual_ratings, test_predictions))\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics_M100 = evaluate(knn_predictions)\n",
    "knn_metrics_M100\n",
    "\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics = evaluate(knn_predictions)\n",
    "knn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 6.897463180889444,\n",
       " 'MAE': 6.7631290634978765,\n",
       " 'Precision@k': 1.0,\n",
       " 'Recall@k': 0.4,\n",
       " 'F1 score': 0.4,\n",
       " 'NDCG': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " knn_metrics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 6.897463180889444,\n",
       " 'MAE': 6.7631290634978765,\n",
       " 'Precision@k': 1.0,\n",
       " 'Recall@k': 0.4,\n",
       " 'F1 score': 0.4,\n",
       " 'NDCG': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics = evaluate(knn_predictions)\n",
    "knn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df = df.copy()  # Add this line\n",
    "\n",
    "    # Create mappings for userIds and movieIds to contiguous indices\n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(df['userId'].unique())}\n",
    "    movie_mapping = {movie_id: i for i, movie_id in enumerate(df['movieId'].unique())}\n",
    "\n",
    "    # Apply the mappings to the dataframe\n",
    "    df.loc[:, 'userId'] = df['userId'].map(user_mapping)\n",
    "    df.loc[:, 'movieId'] = df['movieId'].map(movie_mapping)\n",
    "\n",
    "    # Drop rows with NaN userId or movieId\n",
    "    df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "    # Convert userId and movieId to integer\n",
    "    df.loc[:, 'userId'] = df['userId'].astype(int)\n",
    "    df.loc[:, 'movieId'] = df['movieId'].astype(int)\n",
    "    \n",
    "    return df, user_mapping, movie_mapping\n",
    "\n",
    "\n",
    "\n",
    "def train_knn(df, test_df, k=3):\n",
    "    n_users = df['userId'].nunique()\n",
    "    n_items = df['movieId'].nunique()\n",
    "\n",
    "    train_matrix = df_to_matrix(df, n_users, n_items)\n",
    "    test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "    knn_cf = KNN_CF(n_users, n_items, k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    knn_cf.fit(train_matrix)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "    test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "    actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "    \n",
    "    knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "\n",
    "    # Compute metrics for the KNN model\n",
    "    knn_metrics = evaluate(knn_predictions)\n",
    "\n",
    "    return knn_metrics\n",
    "\n",
    "# Process and train on M100_df\n",
    "M100_df, user_mapping, movie_mapping = process_dataframe(M100_df)\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "knn_metrics_M100 = train_knn(M100_df, test_df)\n",
    "\n",
    "# Process and train on M50_df\n",
    "M50_df, user_mapping, movie_mapping = process_dataframe(M50_df)\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "knn_metrics_M50 = train_knn(M50_df, test_df)\n",
    "\n",
    "# Process and train on M400_df\n",
    "M400_df, user_mapping, movie_mapping = process_dataframe(M400_df)\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "knn_metrics_M400 = train_knn(M400_df, test_df)\n",
    "\n",
    "print(\"KNN Metrics for M100: \", knn_metrics_M100)\n",
    "print(\"KNN Metrics for M50: \", knn_metrics_M50)\n",
    "print(\"KNN Metrics for M400: \", knn_metrics_M400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(df, model, dataset_name)\u001b[0m\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 24\u001b[0m in \u001b[0;36mtrain_knn\u001b[0;34m(df, test_df, k)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_knn\u001b[39m(df, test_df, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Start time\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     n_users \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_items \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def evaluate_model(df, model, dataset_name):\n",
    "    \n",
    "    # Copy df and apply processing\n",
    "    df, user_mapping, movie_mapping = process_dataframe(df)\n",
    "    \n",
    "    # Apply mappings to test_df\n",
    "    test_df_copy = test_df.copy()\n",
    "    test_df_copy['userId'] = test_df_copy['userId'].map(user_mapping)\n",
    "    test_df_copy['movieId'] = test_df_copy['movieId'].map(movie_mapping)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model_metrics = train_knn(df, test_df_copy)\n",
    "    \n",
    "    # Add execution time to metrics\n",
    "    exec_time = time.time() - start_time\n",
    "    model_metrics['Execution time'] = exec_time\n",
    "    \n",
    "    # Add dataset name to metrics\n",
    "    model_metrics['Dataset'] = dataset_name\n",
    "    \n",
    "    # Convert metrics to DataFrame\n",
    "    model_metrics_df = pd.DataFrame([model_metrics])\n",
    "    \n",
    "    return model_metrics_df\n",
    "\n",
    "# Evaluate models for each dataset\n",
    "metrics_M50 = evaluate_model(M50_df, KNN_CF(n_users=M50_df['userId'].nunique(), n_items=M50_df['movieId'].nunique(), k=3), 'M50')\n",
    "metrics_M100 = evaluate_model(M100_df, KNN_CF(n_users=M100_df['userId'].nunique(), n_items=M100_df['movieId'].nunique(), k=3), 'M100')\n",
    "metrics_M400 = evaluate_model(M400_df, KNN_CF(n_users=M400_df['userId'].nunique(), n_items=M400_df['movieId'].nunique(), k=3), 'M400')\n",
    "\n",
    "# Concatenate metrics DataFrames\n",
    "metrics_df = pd.concat([metrics_M50, metrics_M100, metrics_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-2:] + cols[:-2]  # Move the last two columns to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(df, test_df, k=3):\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    n_users = df['userId'].nunique()\n",
    "    n_items = df['movieId'].nunique()\n",
    "\n",
    "    train_matrix = df_to_matrix(df, n_users, n_items)\n",
    "    test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "    knn_cf = KNN_CF(n_users, n_items, k)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    knn_cf.fit(train_matrix)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "    test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "    actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "    # Remove NaN and inf values\n",
    "    clean_predictions = test_predictions[~np.isnan(test_predictions) & ~np.isinf(test_predictions)]\n",
    "    clean_actual_ratings = actual_ratings[~np.isnan(actual_ratings) & ~np.isinf(actual_ratings)]\n",
    "    \n",
    "    knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], clean_actual_ratings, clean_predictions)]\n",
    "\n",
    "    # Compute metrics for the KNN model\n",
    "    knn_metrics = evaluate(knn_predictions)\n",
    "\n",
    "    return knn_metrics\n",
    "\n",
    "\n",
    "def evaluate(predictions, k=10, threshold=3.5):\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=threshold)\n",
    "    f1_scores = f1_score(precisions, recalls)\n",
    "    ndcg = ndcg_at_k(predictions, k=k)\n",
    "\n",
    "    y_true = [true_r for uid, iid, true_r, est in predictions]\n",
    "    y_pred = [est for uid, iid, true_r, est in predictions]\n",
    "\n",
    "    # Remove NaN and inf values\n",
    "    y_true_clean = np.array(y_true)[~np.isnan(y_true) & ~np.isinf(y_true)]\n",
    "    y_pred_clean = np.array(y_pred)[~np.isnan(y_pred) & ~np.isinf(y_pred)]\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse(y_true_clean, y_pred_clean),\n",
    "        'MAE': mae(y_true_clean, y_pred_clean),\n",
    "        'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "        'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "        'F1 score': sum(f1 for f1 in f1_scores.values()) / len(f1_scores),\n",
    "        'NDCG': ndcg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m M100_df, user_mapping, movie_mapping \u001b[39m=\u001b[39m process_dataframe(M100_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_df_M100 \u001b[39m=\u001b[39m map_and_dropna(test_df, user_mapping, movie_mapping)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m knn_metrics_M100 \u001b[39m=\u001b[39m train_knn(M100_df, test_df_M100)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Process and train on M50_df\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m M50_df, user_mapping, movie_mapping \u001b[39m=\u001b[39m process_dataframe(M50_df)\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain_knn\u001b[0;34m(df, test_df, k)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_knn\u001b[39m(df, test_df, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Start time\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     n_users \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy.ipynb#Y103sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     n_items \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "def map_and_dropna(df, user_mapping, movie_mapping):\n",
    "    df = df.copy()\n",
    "    df['userId'] = df['userId'].map(user_mapping)\n",
    "    df['movieId'] = df['movieId'].map(movie_mapping)\n",
    "    df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "    \n",
    "    # Convert to integer type if they are not NaN\n",
    "    df.loc[df['userId'].notna(), 'userId'] = df.loc[df['userId'].notna(), 'userId'].astype(int)\n",
    "    df.loc[df['movieId'].notna(), 'movieId'] = df.loc[df['movieId'].notna(), 'movieId'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Process and train on M100_df\n",
    "M100_df, user_mapping, movie_mapping = process_dataframe(M100_df)\n",
    "test_df_M100 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M100 = train_knn(M100_df, test_df_M100)\n",
    "\n",
    "# Process and train on M50_df\n",
    "M50_df, user_mapping, movie_mapping = process_dataframe(M50_df)\n",
    "test_df_M50 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M50 = train_knn(M50_df, test_df_M50)\n",
    "\n",
    "# Process and train on M400_df\n",
    "M400_df, user_mapping, movie_mapping = process_dataframe(M400_df)\n",
    "test_df_M400 = map_and_dropna(test_df, user_mapping, movie_mapping)\n",
    "knn_metrics_M400 = train_knn(M400_df, test_df_M400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
